{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Projects\\AI_QA\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Please install `datasets` to use this function: `pip install datasets`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m train_loss = losses.CosineSimilarityLoss(model)\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Fine-tune model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_objectives\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Save the fine-tuned model\u001b[39;00m\n\u001b[32m     25\u001b[39m model_save_path = \u001b[33m\"\u001b[39m\u001b[33msbert_finetuned\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Projects\\AI_QA\\myenv\\Lib\\site-packages\\sentence_transformers\\fit_mixin.py:243\u001b[39m, in \u001b[36mFitMixin.fit\u001b[39m\u001b[34m(self, train_objectives, evaluator, epochs, steps_per_epoch, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback, show_progress_bar, checkpoint_path, checkpoint_save_steps, checkpoint_save_total_limit)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[33;03mDeprecated training method from before Sentence Transformers v3.0, it is recommended to use\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[33;03m:class:`~sentence_transformers.trainer.SentenceTransformerTrainer` instead. This method uses\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    240\u001b[39m \u001b[33;03m        store\u001b[39;00m\n\u001b[32m    241\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_datasets_available():\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mPlease install `datasets` to use this function: `pip install datasets`.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    245\u001b[39m \u001b[38;5;66;03m# Delayed import to counter the SentenceTransformers -> FitMixin -> SentenceTransformerTrainer -> SentenceTransformers circular import\u001b[39;00m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformerTrainer\n",
      "\u001b[31mImportError\u001b[39m: Please install `datasets` to use this function: `pip install datasets`."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"ml_trainingques.csv\")  # Ensure the dataset is in the same directory\n",
    "\n",
    "data_samples = []\n",
    "for _, row in df.iterrows():\n",
    "    data_samples.append(InputExample(texts=[row[\"question\"], row[\"answer\"]], label=float(row[\"score\"])))\n",
    "\n",
    "# Load a pretrained SBERT model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Prepare DataLoader\n",
    "dataloader = DataLoader(data_samples, batch_size=8, shuffle=True)\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "# Fine-tune model\n",
    "model.fit(train_objectives=[(dataloader, train_loss)], epochs=10, warmup_steps=100)\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model_save_path = \"sbert_finetuned\"\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "model.save(model_save_path)\n",
    "\n",
    "print(f\"Model saved at {model_save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
